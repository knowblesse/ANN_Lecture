{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Hw08] Convolutional Neural Network\n",
    "Jeong Ji Hoon 2016010980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the MNIST train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수업 자료의 코드를 그대로 사용.\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "# unzips mnist\n",
    "if (sys.version_info > (3, 0)):\n",
    "    writemode = 'wb'\n",
    "else:\n",
    "    writemode = 'w'\n",
    "\n",
    "zipped_mnist = [f for f in os.listdir('./') if f.endswith('ubyte.gz')]\n",
    "for z in zipped_mnist:\n",
    "    with gzip.GzipFile(z, mode='rb') as decompressed, open(z[:-3], writemode) as outfile:\n",
    "        outfile.write(decompressed.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set training & validation & test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 55000 개와 5000개로 traning set을 나눔.\n",
    "\n",
    "X, y = load_mnist('', kind='train')\n",
    "\n",
    "X_train = X[:55000,:]\n",
    "y_train = y[:55000]\n",
    "X_valid = X[55000:60000,:]\n",
    "y_valid = y[55000:60000]\n",
    "\n",
    "X_test, y_test = load_mnist('', kind='t10k')\n",
    "X_test = X_test[:10000,:]\n",
    "y_test = y_test[:10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_valid_centered = X_valid - mean_vals\n",
    "X_test_centered = (X_test - mean_vals) / std_val\n",
    "\n",
    "del X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create and Evaluate CNN with low-level tensorflow API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Define functions for layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolution layer (not changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_tensor, name,\n",
    "               kernel_size, n_output_channels,\n",
    "               padding_mode='SAME', strides=(1, 1, 1, 1)):\n",
    "    with tf.variable_scope(name):\n",
    "        ## get n_input_channels:\n",
    "        ##   input tensor shape:\n",
    "        ##   [batch x width x height x channels_in]\n",
    "        input_shape = input_tensor.get_shape().as_list()\n",
    "        n_input_channels = input_shape[-1]\n",
    "\n",
    "        weights_shape = (list(kernel_size) +\n",
    "                         [n_input_channels, n_output_channels])\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_channels]))\n",
    "        print(biases)\n",
    "        conv = tf.nn.conv2d(input=input_tensor,\n",
    "                            filter=weights,\n",
    "                            strides=strides,\n",
    "                            padding=padding_mode)\n",
    "        print(conv)\n",
    "        conv = tf.nn.bias_add(conv, biases,\n",
    "                              name='net_pre-activation')\n",
    "        print(conv)\n",
    "        conv = tf.nn.relu(conv, name='activation')\n",
    "        print(conv)\n",
    "\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fully connected layer (not changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(input_tensor, name,\n",
    "             n_output_units, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units = np.prod(input_shape)\n",
    "        if len(input_shape) > 1:\n",
    "            input_tensor = tf.reshape(input_tensor,\n",
    "                                      shape=(-1, n_input_units))\n",
    "\n",
    "        weights_shape = [n_input_units, n_output_units]\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_units]))\n",
    "        print(biases)\n",
    "        layer = tf.matmul(input_tensor, weights)\n",
    "        print(layer)\n",
    "        layer = tf.nn.bias_add(layer, biases,\n",
    "                               name='net_pre-activation')\n",
    "        print(layer)\n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "\n",
    "        layer = activation_fn(layer, name='activation')\n",
    "        print(layer)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Define the function for Creating CNN\n",
    "- <font color='red' >**기존 코드에는 L1 xor L2 regularization term이 없음**</font>\n",
    "- 이에 LnReg parameter로 regularization을 할지, 한다면 L1으로 할지 L2로 할지를 받음.\n",
    "- LnReg 값이 1 혹은 2로 설정이 되어 각각 L1 혹은 L2 regularization이 penalty term으로 들어간다면, LnBeta 파라미터에 람다 값을 넣어줌.\n",
    "- L1의 경우 abs를 한뒤 reduced sum을 시켜서 절대값 취한뒤 합해주는 방식을 취함.\n",
    "- L2의 경우는 자체 함수인 tf.nn.l2_loss 함수를 사용함.\n",
    "- 과제 지시에 따라 모든 weight에 대해 penalize 하지 않고 첫번째 fully connected layer인 'fc_3'에 대해서만 penalize를 진행.\n",
    "- 추가로 LnReg 값으로 0, 1, 2가 아닌 다른 값을 집어 넣으면 raise error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(LnReg = 0, LnBeta = 0):\n",
    "    # L1 혹은 L2 regularization을 위해서 만든 두 parameter.\n",
    "    # Param\n",
    "    #   LnReg = 0, 1, 2\n",
    "    #       0인 경우 Regularization을 하지 않고\n",
    "    #       1인 경우 L1,\n",
    "    #       2인 경우 L2 Regularization term을 loss function에 추가.\n",
    "    #   LnBeta = float\n",
    "    #       LnReg 가 1 혹은 2인 경우 beta 값으로 사용.\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, 784],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor:\n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    h1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling\n",
    "    h1_pool = tf.nn.max_pool(h1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    h2 = conv_layer(h1_pool, name='conv_2',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    ## MaxPooling\n",
    "    h2_pool = tf.nn.max_pool(h2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Fully Connected\n",
    "    print('\\nBuilding 3rd layer:')\n",
    "    h3 = fc_layer(h2_pool, name='fc_3',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    h3_drop = tf.nn.dropout(h3, keep_prob=keep_prob,\n",
    "                            name='dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    h4 = fc_layer(h3_drop, name='fc_4',\n",
    "                  n_output_units=10,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities': tf.nn.softmax(h4, name='probabilities'),\n",
    "        'labels': tf.cast(tf.argmax(h4, axis=1), tf.int32,\n",
    "                          name='labels')\n",
    "    }\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    if LnReg == 0: # Ln Regularization 없음.\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=h4, labels=tf_y_onehot),\n",
    "            name='cross_entropy_loss')\n",
    "    elif LnReg == 1: # L1 Regularization\n",
    "        with tf.variable_scope('fc_3',reuse=True):\n",
    "            ln_values = tf.get_variable('_weights')\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=h4, labels=tf_y_onehot)\n",
    "            + LnBeta*tf.reduce_sum(tf.abs(ln_values)),\n",
    "            name='cross_entropy_loss')\n",
    "    elif LnReg == 2: # L2 Regularization\n",
    "        with tf.variable_scope('fc_3',reuse=True):\n",
    "            ln_values = tf.get_variable('_weights')\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=h4, labels=tf_y_onehot)\n",
    "            + LnBeta*tf.nn.l2_loss(ln_values),\n",
    "            name='cross_entropy_loss')\n",
    "    else:\n",
    "        raise Exception('LnReg value must be either 0, 1, 2')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss, name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. Define helper functions\n",
    "- 다른 함수들은 그대로 두고 train 함수에서 batch_size를 따로 받을 수 있도록 변경."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=64,\n",
    "                    shuffle=False, random_seed=None):\n",
    "    idx = np.arange(y.shape[0])\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X[i:i + batch_size, :], y[i:i + batch_size])\n",
    "\n",
    "        \n",
    "def save(saver, sess, epoch, path='./model/'):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    print('Saving model in %s' % path)\n",
    "    saver.save(sess, os.path.join(path, 'cnn-model.ckpt'),\n",
    "               global_step=epoch)\n",
    "\n",
    "\n",
    "def load(saver, sess, path, epoch):\n",
    "    print('Loading model from %s' % path)\n",
    "    saver.restore(sess, os.path.join(\n",
    "        path, 'cnn-model.ckpt-%d' % epoch))\n",
    "\n",
    "\n",
    "def train(sess, training_set, validation_set=None,\n",
    "          initialize=True, epochs=20, shuffle=True,\n",
    "          dropout=0.5, batch_size=64, random_seed=None):\n",
    "    X_data = np.array(training_set[0])\n",
    "    y_data = np.array(training_set[1])\n",
    "    training_loss = []\n",
    "\n",
    "    ## initialize variables\n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(random_seed)  # for shuflling in batch_generator\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        batch_gen = batch_generator(\n",
    "            X_data, y_data, batch_size=batch_size,\n",
    "            shuffle=shuffle)\n",
    "        avg_loss = 0.0\n",
    "        for i, (batch_x, batch_y) in enumerate(batch_gen):\n",
    "            feed = {'tf_x:0': batch_x,\n",
    "                    'tf_y:0': batch_y,\n",
    "                    'fc_keep_prob:0': dropout}\n",
    "            loss, _ = sess.run(\n",
    "                ['cross_entropy_loss:0', 'train_op'],\n",
    "                feed_dict=feed)\n",
    "            avg_loss += loss\n",
    "\n",
    "        training_loss.append(avg_loss / (i + 1))\n",
    "        print('Epoch %02d Training Avg. Loss: %7.3f' % (\n",
    "            epoch, avg_loss), end=' ')\n",
    "        if validation_set is not None:\n",
    "            feed = {'tf_x:0': validation_set[0],\n",
    "                    'tf_y:0': validation_set[1],\n",
    "                    'fc_keep_prob:0': 1.0}\n",
    "            valid_acc = sess.run('accuracy:0', feed_dict=feed)\n",
    "            print(' Validation Acc: %7.3f' % valid_acc)\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "\n",
    "def predict(sess, X_test, return_proba=False):\n",
    "    feed = {'tf_x:0': X_test,\n",
    "            'fc_keep_prob:0': 1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities:0', feed_dict=feed)\n",
    "    else:\n",
    "        return sess.run('labels:0', feed_dict=feed)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define hyperparameter sets\n",
    "param_dropout = [0.1, 0.5, 1.0]\n",
    "param_L2 = [0, 0.1, 0.001]\n",
    "param_L1 = [0, 0.1, 0.001]\n",
    "param_batch = [1,4,16,64,128,1024,55000]\n",
    "\n",
    "\n",
    "## Define fixed hyperparameters\n",
    "learning_rate = 1e-4\n",
    "random_seed = 123\n",
    "epoch = 10\n",
    "\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. Dropout Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn(LnReg = 0, LnBeta = 0)\n",
    "    ## saver:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "## crearte a TF session and train the CNN model\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    train(sess,\n",
    "          training_set=(X_train_centered, y_train),\n",
    "          validation_set=(X_valid_centered, y_valid),\n",
    "          initialize=True,\n",
    "          epochs=epoch,\n",
    "          shuffle=True,\n",
    "          dropout=0.5,\n",
    "          batch_size=64,\n",
    "          random_seed=123)\n",
    "    save(saver, sess, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
